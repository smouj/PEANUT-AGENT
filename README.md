# ğŸ¤– Agente Local con Ollama - Sistema Completo

Sistema que hace que **modelos pequeÃ±os (7B-8B)** funcionen tan bien como modelos grandes usando tool calling + arquitectura optimizada.

## ğŸ¯ Lo que hace diferente a este sistema

| Problema del modelo pequeÃ±o | SoluciÃ³n implementada |
|------------------------------|----------------------|
| Se pierde con muchas herramientas | âœ… Solo 7 herramientas, ultra-claras |
| Rompe el JSON de tool calls | âœ… ValidaciÃ³n + auto-correcciÃ³n |
| No sabe quÃ© archivos existen | âœ… Contexto enriquecido automÃ¡tico |
| Se inventa argumentos | âœ… JSON Schema estricto |
| Es inconsistente | âœ… Temperatura 0 = cero creatividad |

## ğŸ“¦ InstalaciÃ³n

### 1. Instalar Ollama

```bash
# Linux / WSL
curl -fsSL https://ollama.com/install.sh | sh

# macOS
brew install ollama
```

### 2. Descargar un modelo

```bash
# Recomendado: Qwen 2.5 (mejor para tool calling)
ollama pull qwen2.5:7b

# Alternativas
ollama pull mistral:7b-instruct
ollama pull llama3.2:3b
```

### 3. Instalar dependencias Python

```bash
pip install requests
```

## ğŸš€ Uso rÃ¡pido

### Modo bÃ¡sico (un comando)

```python
from agent import OllamaAgent

# Crear agente
agent = OllamaAgent(model="qwen2.5:7b")

# Ejecutar tarea
response = agent.run("Lista los archivos Python del directorio actual")
print(response)
```

### Modo interactivo (chat)

```bash
python agent.py
```

Esto inicia un chat donde puedes conversar con el agente:

```
ğŸ‘¤ TÃº: Crea un archivo README.md con un tÃ­tulo y descripciÃ³n del proyecto
ğŸ¤– Agente: [ejecuta write_file] âœ“ He creado el archivo README.md

ğŸ‘¤ TÃº: Ahora sÃºbelo a git con el mensaje "Initial commit"
ğŸ¤– Agente: [ejecuta git add, git commit] âœ“ Commit creado
```

## ğŸ› ï¸ Herramientas disponibles

### 1. **shell** - Ejecutar comandos seguros

```python
# Ejemplos permitidos
"Lista archivos: ls -la"
"Buscar texto: grep 'error' logs.txt"
"Ejecutar script: python3 script.py"

# âŒ Prohibidos: rm, sudo, shutdown, etc.
```

### 2. **read_file** - Leer archivos

```python
"Lee el contenido de package.json"
"QuÃ© dice el archivo config.py?"
```

### 3. **write_file** - Escribir archivos

```python
"Crea un archivo test.py con un hello world"
"Guarda este JSON en data.json: {...}"
```

### 4. **list_directory** - Listar directorios

```python
"QuÃ© archivos hay en el directorio actual?"
"Lista el contenido de ./src"
```

### 5. **http_request** - Peticiones HTTP

```python
"Haz un GET a https://api.github.com/users/octocat"
"POST a https://httpbin.org/post con body: {\"test\": true}"
```

### 6. **git** - Operaciones Git

```python
"Muestra el estado de git"
"Haz commit de todos los cambios con mensaje 'Update docs'"
"Push a la rama main"
```

### 7. **docker** - Operaciones Docker

```python
"Muestra los contenedores corriendo"
"Levanta docker-compose"
"Muestra los logs del servicio web"
```

## ğŸ“š Ejemplos completos

### Ejemplo 1: AnÃ¡lisis de proyecto

```python
from agent import OllamaAgent

agent = OllamaAgent(model="qwen2.5:7b")

response = agent.run("""
Analiza este proyecto:
1. Lista todos los archivos .py
2. Lee el package.json si existe
3. Muestra el estado de git
4. Dame un resumen del proyecto
""")

print(response)
```

### Ejemplo 2: AutomatizaciÃ³n de despliegue

```python
agent = OllamaAgent(model="qwen2.5:7b")

response = agent.run("""
Despliega la aplicaciÃ³n:
1. Verifica que no haya cambios sin commitear (git status)
2. Ejecuta los tests (python -m pytest)
3. Si pasan, haz build (npm run build)
4. Levanta docker-compose
5. Verifica que el servicio web estÃ© corriendo
""")

print(response)
```

### Ejemplo 3: InvestigaciÃ³n de API

```python
agent = OllamaAgent(model="qwen2.5:7b")

response = agent.run("""
Investiga la API de GitHub:
1. Haz GET a https://api.github.com/repos/anthropics/anthropic-sdk-python
2. Extrae: nombre, estrellas, lenguaje, Ãºltima actualizaciÃ³n
3. Guarda los resultados en github_info.json
""")

print(response)
```

## âš™ï¸ ConfiguraciÃ³n avanzada

### Cambiar modelo

```python
agent = OllamaAgent(
    model="mistral:7b-instruct",  # o el que prefieras
    temperature=0.0
)
```

### Ajustar temperatura (creatividad)

```python
# Tareas operativas (default)
agent = OllamaAgent(temperature=0.0)

# Generar cÃ³digo/docs
agent = OllamaAgent(temperature=0.3)

# Brainstorming
agent = OllamaAgent(temperature=0.7)
```

### Cambiar directorio de trabajo

```python
agent = OllamaAgent(work_dir="/home/usuario/proyecto")
```

### LÃ­mite de iteraciones

```python
agent = OllamaAgent(max_iterations=20)  # default: 10
```

## ğŸ—ï¸ Arquitectura del sistema

```
Usuario
  â†“
  input: "Lista archivos Python"
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OllamaAgent                 â”‚
â”‚ + Contexto enriquecido      â”‚ â†’ "ğŸ“‚ Dir: /home/user, ğŸ“„ Archivos: x.py, y.py"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama API                  â”‚
â”‚ + JSON Schema (tools)       â”‚ â†’ Modelo propone: shell("ls *.py")
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ToolExecutor                â”‚
â”‚ + Allowlist validation      â”‚ â†’ âœ“ "ls" estÃ¡ permitido
â”‚ + Security checks           â”‚ â†’ âœ— "rm" estÃ¡ prohibido
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
  Ejecuta: subprocess.run("ls *.py")
  â†“
  Resultado: ["script.py", "agent.py"]
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Validation + Retry          â”‚
â”‚ Â¿JSON vÃ¡lido? â†’ SÃ­ âœ“        â”‚
â”‚ Â¿Error? â†’ Reintentar        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
  Respuesta final al usuario
```

## ğŸ” Seguridad

### Allowlist de comandos shell

Solo se permiten comandos seguros:
- âœ… Lectura: `ls`, `cat`, `grep`, `find`
- âœ… Desarrollo: `python`, `npm`, `git`, `docker`
- âŒ Prohibidos: `rm`, `sudo`, `shutdown`, `chmod`

### Path traversal protection

Todos los archivos se validan contra el directorio de trabajo:
```python
# âŒ Bloqueado: ../../../etc/passwd
# âœ… Permitido: ./config.json
```

### Timeouts

Todos los comandos tienen timeout:
- Shell: 30 segundos
- HTTP: 30 segundos
- Docker: 60 segundos

## ğŸ› Debugging

### Ver quÃ© estÃ¡ haciendo el agente

```python
agent = OllamaAgent(model="qwen2.5:7b")
response = agent.run("tu comando", verbose=True)
```

Output:
```
============================================================
ğŸ”„ IteraciÃ³n 1/10
============================================================
ğŸ”§ Herramientas solicitadas: 1

â–¶ï¸  Ejecutando: shell
   Args: {"cmd": "ls -la"}
   âœ“ Resultado: {"stdout": "total 24\n-rw-r--r-- 1..."}

============================================================
âœ… Respuesta final:
He listado los archivos...
```

### Ver historial de mensajes

```python
history = agent.get_history()
for msg in history:
    print(f"{msg['role']}: {msg.get('content', 'tool_call')[:50]}")
```

## ğŸ“ Mejores prÃ¡cticas

### 1. Tareas especÃ­ficas y claras

```python
# âŒ Ambiguo
"Haz algo con los archivos"

# âœ… Claro
"Lista todos los archivos .py, lee cada uno, y crea un resumen en summary.txt"
```

### 2. Usar temperatura 0 para operaciones

```python
# Para tareas operativas (archivos, comandos)
agent = OllamaAgent(temperature=0.0)

# Para creatividad (generar cÃ³digo, ideas)
agent = OllamaAgent(temperature=0.3)
```

### 3. Proveer contexto

```python
# âœ… Mejor
"Este es un proyecto Flask. Lista las rutas definidas en app.py y crea documentaciÃ³n en API.md"

# vs
"Lista las rutas"
```

### 4. Verificar resultados intermedios

```python
# El agente puede verificar sus propios pasos
agent.run("""
1. Crea test.txt con contenido "Hello"
2. Lee test.txt para verificar
3. Si estÃ¡ correcto, responde OK
""")
```

## ğŸ”§ Troubleshooting

### Problema: "Error llamando a Ollama"

```bash
# Verificar que Ollama estÃ© corriendo
ollama list

# Iniciar Ollama si no estÃ¡
ollama serve
```

### Problema: "Modelo no responde"

```bash
# Verificar que el modelo estÃ© descargado
ollama pull qwen2.5:7b

# Probar el modelo manualmente
ollama run qwen2.5:7b "Hola"
```

### Problema: "Tool calling no funciona bien"

1. Usa `qwen2.5:7b` (mejor para tool calling)
2. Temperatura = 0.0
3. Tareas especÃ­ficas (no ambiguas)

### Problema: "JSONDecodeError"

El sistema tiene auto-correcciÃ³n, pero si persiste:
- Usa un modelo mejor (qwen2.5:14b)
- Simplifica la tarea
- Reduce nÃºmero de herramientas simultÃ¡neas

## ğŸ“Š ComparaciÃ³n de modelos

| Modelo | TamaÃ±o | Tool Calling | Velocidad | RAM |
|--------|--------|--------------|-----------|-----|
| qwen2.5:7b | 7B | â­â­â­â­â­ | RÃ¡pido | 8GB |
| qwen2.5:14b | 14B | â­â­â­â­â­ | Medio | 16GB |
| mistral:7b | 7B | â­â­â­â­ | RÃ¡pido | 8GB |
| llama3.2:3b | 3B | â­â­â­ | Muy rÃ¡pido | 4GB |
| phi3:mini | 3.8B | â­â­â­ | Muy rÃ¡pido | 4GB |

## ğŸ¤ Contribuir

Puedes aÃ±adir mÃ¡s herramientas editando `tools.py`:

```python
def _mi_herramienta(self, args: Dict[str, Any]) -> Dict[str, Any]:
    """Tu herramienta personalizada"""
    # Tu cÃ³digo aquÃ­
    return {"result": "..."}
```

Y agregÃ¡ndola a `TOOLS_SCHEMA` con su JSON Schema.

## ğŸ“„ Licencia

MIT

## ğŸ™ CrÃ©ditos

Basado en la filosofÃ­a de:
- Tool calling de Anthropic
- Guided decoding de vLLM
- Structured outputs de Ollama

---

**Â¿Preguntas?** Abre un issue o consulta la documentaciÃ³n de [Ollama](https://docs.ollama.com/).
